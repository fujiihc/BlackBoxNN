{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Adjust as needed for project versions and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa as lb\n",
    "import soundfile as sf\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleRate = 44100\n",
    "channels = 1\n",
    "\n",
    "\n",
    "learningRate = [0.01, 0.001, 0.0001]\n",
    "\n",
    "#samples relative to sampleRate\n",
    "#44 = 1ms at 44.1K Hz\n",
    "context = [44, 66, 88]\n",
    "\n",
    "#recorded gain values\n",
    "gain = [\"Low\", \"Mid\", \"High\"]\n",
    "\n",
    "lstmUnits = [64, 96]\n",
    "\n",
    "#SD-1\n",
    "effect = \"\"\n",
    "#DS340\n",
    "projVer = \"\"\n",
    "\n",
    "patience = 15\n",
    "batch = 64\n",
    "epoch = 100\n",
    "shuf = True\n",
    "\n",
    "state = 25\n",
    "np.random.seed(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esrLoss(true, pred):\n",
    "    return tf.reduce_sum(tf.pow(true - pred,2), axis = 0) / (tf.reduce_sum(tf.pow(true, 2), axis = 0) + 1e-50)\n",
    "\n",
    "def createModel(units, rate):\n",
    "    #mess with input shapes later\n",
    "    inputLayer = Input(shape = (None,1))\n",
    "    lstm = LSTM(units)(inputLayer)\n",
    "    fullyConnected = Dense(1)(lstm)\n",
    "    model = Model(inputs = inputLayer, outputs = fullyConnected)\n",
    "    model.compile(optimizer = Adam(learning_rate = rate), loss = esrLoss, metrics = [\"mae\", \"mse\"])\n",
    "    return model\n",
    "\n",
    "def predictionPrep(audio, context):\n",
    "    chunks = []\n",
    "    for i in range(len(audio) - context):\n",
    "        chunks.append(audio[i:i+context])\n",
    "    return np.array(chunks)\n",
    "\n",
    "def evaluateNew(model, xTest, yTest, context):\n",
    "    yPred = model.predict(predictionPrep(xTest, context))\n",
    "    return esrLoss(yTest[context:], yPred.squeeze()), yPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reference: https://rcs.bu.edu/examples/machine_learning/\n",
    "gpuIds = os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "\n",
    "modelNum = 0\n",
    "\n",
    "for lvl in gain:\n",
    "    for size in context:\n",
    "    \n",
    "\n",
    "        with open(f\"../TrainValPickles/{effect}/{projVer}/{lvl}_{size}_x.pkl\", \"rb\") as file1:\n",
    "            x = pickle.load(file1)\n",
    "\n",
    "        with open(f\"../TrainValPickles/{effect}/{projVer}/{lvl}_{size}_y.pkl\", \"rb\") as file2:\n",
    "            y = pickle.load(file2) \n",
    "        \n",
    "      \n",
    "        #these should be adjusted with the size of the training data\n",
    "        testSize = 0.0\n",
    "        if size == 44:\n",
    "            testSize = 0.6\n",
    "        if size == 66:\n",
    "            testSize = 0.4\n",
    "        if size == 88:\n",
    "            testSize = 0.2\n",
    "\n",
    "        #add a way to train on the WHOLE dataset\n",
    "        #use external validation data\n",
    "        xTrain, xVal, yTrain, yVal = train_test_split(x, y, test_size = testSize, random_state = state)\n",
    "    \n",
    "        #evaluate on shadowLove\n",
    "        evalDry, _ = lb.load(f\"../Data/{effect}/{projVer}/Test/{lvl}/Dry/d_shadowlove.wav\", sr=sampleRate, mono=True)\n",
    "        evalWet, _ = lb.load(f\"../Data/{effect}/{projVer}/Test/{lvl}/Wet/w_shadowlove.wav\", sr=sampleRate, mono=True)\n",
    "        \n",
    "        \n",
    "        for units in lstmUnits:\n",
    "            for rate in learningRate:\n",
    "                \n",
    "                clear_session()\n",
    "                directory = f\"../Models/{effect}/{projVer}/Model_{modelNum}\"\n",
    "                model = createModel(units, rate)\n",
    "\n",
    "                os.makedirs(directory, exist_ok=True)\n",
    "                #optional for monitoring model\n",
    "                \"\"\"\n",
    "                with open(f\"{directory}/modelSummary_{modelNum}.txt\", \"w\") as file1:\n",
    "                    sys.stdout = file1\n",
    "                    model.summary()  \n",
    "                    sys.stdout = sys.__stdout__\n",
    "                \"\"\"\n",
    "                earlyStop = EarlyStopping(monitor = 'val_loss', patience = patience, restore_best_weights = True)\n",
    "                start = time.time()\n",
    "                #use test to run Validation, will use my data for testing\n",
    "                history = model.fit(xTrain, yTrain, validation_data = (xVal, yVal), batch_size = batch, epochs = epoch, callbacks = [earlyStop], shuffle = shuf)\n",
    "                end = time.time()\n",
    "                trainTime = end - start\n",
    "\n",
    "                testing, output = evaluateNew(model, evalDry, evalWet, size)\n",
    "\n",
    "                parameters = {\n",
    "                    \"sampleRate\" : int(sampleRate),\n",
    "                    \"contextSize\" : int(size),\n",
    "                    \"trainNvalSize\" : xTrain.shape[0],\n",
    "                    \"lstmUnits\" : int(units),\n",
    "                    \"learningRate\" : float(rate),\n",
    "                    \"trainEpochs\" : int(epoch),\n",
    "                    \"epochs\" : len(history.history[\"loss\"]),\n",
    "                    \"patience\" : int(patience),\n",
    "                    \"batchSize\" : int(batch),\n",
    "                    \"shuffle\" : shuf,\n",
    "                    \"randomState\" : int(state),\n",
    "                    \"trainTime\": trainTime,\n",
    "                    \"gain\" : str(lvl),\n",
    "                    \"testESR\" : float(testing.numpy())\n",
    "                }\n",
    "\n",
    "\n",
    "                modelName = f\"{lvl}_{units}_{rate}_{size}_{modelNum}\"\n",
    "                #or use PCM_16 or something\n",
    "                sf.write(f\"{directory}/{modelName}.wav\", output, sampleRate, subtype = \"PCM_24\")\n",
    "                \n",
    "                model.save(f\"{directory}/model_{modelName}.h5\")\n",
    "\n",
    "                with open(f\"{directory}/history_{modelName}.json\", \"w\") as file3:\n",
    "                    json.dump(history.history, file3)\n",
    "\n",
    "                with open(f\"{directory}/parameters_{modelName}.json\", \"w\") as file4:\n",
    "                    json.dump(parameters, file4)\n",
    "\n",
    "                #print(\"finished model \" + modelName)\n",
    "                modelNum += 1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
