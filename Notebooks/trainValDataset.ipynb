{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Training/Validation Datasets\n",
    "Adjust as needed for project versions and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import librosa as lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleRate = 44100\n",
    "channels = 1\n",
    "repalce = False\n",
    "\n",
    "gain = [\"Low\", \"Mid\", \"High\"]\n",
    "context = [44, 66, 88]\n",
    "\n",
    "effect = \"SD-1\"\n",
    "projVer = \"DS340\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeAudio(data):\n",
    "    maxAll = np.max(np.abs(data))\n",
    "    return data / maxAll\n",
    "\n",
    "#use gap, and gapSize experiments later\n",
    "def splitChunks(dry, wet, context):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(0, len(dry) - context, context):\n",
    "        seq = dry[i:i+context]\n",
    "        out = wet[i+context-1]\n",
    "        x.append(seq)\n",
    "        y.append(out)\n",
    "    return x, y\n",
    "\n",
    "#you don't actually really need this\n",
    "#might rework this one\n",
    "def shrinkData(x, y, trainSize, replace = False):\n",
    "    indexes = np.random.choice(x.shape[0], trainSize, replace = replace)\n",
    "    return x[indexes], y[indexes]\n",
    "\n",
    "\n",
    "def loadData(context, gain):\n",
    "    drys = []\n",
    "    wets = []\n",
    "    for _, _, files in os.walk(f\"../Data/{effect}/{projVer}/Train/{gain}/Dry\"):\n",
    "        for file in files:\n",
    "            fileName = file[2:]\n",
    "            dry, _ = lb.load(f\"../Data/{effect}/{projVer}/Train/{gain}/Dry/d_\" + fileName, sr=sampleRate, mono=True)\n",
    "            wet, _ = lb.load(f\"../Data/{effect}/{projVer}/Train/{gain}/Wet/w_\" + fileName, sr=sampleRate, mono=True)\n",
    "            x, y = splitChunks(dry, wet, context)\n",
    "            drys += x\n",
    "            wets += y\n",
    "    drys = np.array(drys)\n",
    "    wets = np.array(wets)\n",
    "\n",
    "    directory = f\"../TrainValPickles/{effect}/{projVer}\"\n",
    "    \n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    with open(f\"{directory}/{gain}_{context}_x.pkl\", \"wb\") as file1:\n",
    "            pickle.dump(drys, file1)\n",
    "\n",
    "    with open(f\"{directory}/{gain}_{context}_y.pkl\", \"wb\") as file2:\n",
    "            pickle.dump(wets, file2)\n",
    "\n",
    "    return drys, wets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lvl in gain:\n",
    "    for size in context:\n",
    "        x, y = loadData(size, lvl)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
